{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huzi/Desktop/RAG/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76de0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={}, client=<groq.resources.chat.completions.Completions object at 0x12b558e80>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x12b558910>, model_name='meta-llama/Llama-4-Scout-17B-16E-Instruct', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To tackle the complex question of how LangChain uses memory and agents compared to CrewAI, we can break it down into more manageable sub-questions. Here are 3 sub-questions for better document retrieval:\n",
      "\n",
      "1. **How does LangChain utilize memory in its operations?**\n",
      "   - This sub-question aims to understand the specific mechanisms, techniques, or architectures LangChain employs for memory usage.\n",
      "\n",
      "2. **How does LangChain implement and use agents in its framework?**\n",
      "   - This sub-question seeks to clarify the role of agents within LangChain, including their design, functionality, and integration with other components.\n",
      "\n",
      "3. **How do CrewAI's approaches to memory and agents differ from or compare to LangChain's?**\n",
      "   - Alternatively, this could be split into two sub-questions for a more granular comparison:\n",
      "   - **How does CrewAI utilize memory in its operations?**\n",
      "   - **How does CrewAI implement and use agents in its framework?**\n",
      "\n",
      "By answering these sub-questions, one can gather detailed insights that facilitate a comprehensive comparison between LangChain and CrewAI regarding their use of memory and agents.\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: To tackle the complex question of how LangChain uses memory and agents compared to CrewAI, we can break it down into more manageable sub-questions. Here are 3 sub-questions that can help in better document retrieval:\n",
      "A: Based on the provided context, here are 3 sub-questions that can help in better understanding how LangChain uses memory and agents compared to CrewAI:\n",
      "\n",
      "1. **How does LangChain integrate with vector databases, and what benefits does this provide for semantic search and Retrieval-Augmented Generation (RAG)?**\n",
      "\n",
      "This sub-question can help clarify LangChain's approach to memory and knowledge retrieval, specifically its use of vector databases like FAISS, Chroma, Pinecone, and Weaviate.\n",
      "\n",
      "2. **What role does LangChain play in handling retrieval and tool wrapping in hybrid systems with CrewAI, and how does CrewAI manage role-based collaboration?**\n",
      "\n",
      "This sub-question can help understand the division of labor between LangChain and CrewAI in hybrid systems, and how they complement each other in terms of memory and agent management.\n",
      "\n",
      "3. **How does LangChain's support for hybrid retrieval (combining keyword-based and embedding-based methods) impact its ability to recall relevant information, and what implications does this have for its use of memory and agents?**\n",
      "\n",
      "This sub-question can help shed light on LangChain's approach to memory and retrieval, specifically its use of hybrid retrieval methods to improve recall and accuracy.\n",
      "\n",
      "Q: **How does LangChain utilize memory in its operations?**\n",
      "A: LangChain utilizes memory through its memory modules, specifically:\n",
      "\n",
      "1. **ConversationBufferMemory**: This module allows the LLM (Large Language Model) to maintain awareness of previous conversation turns.\n",
      "2. **ConversationSummaryMemory**: This module enables the LLM to summarize long interactions to fit within token limits.\n",
      "\n",
      "These memory modules enable the LLM to retain context and understanding of the conversation history, facilitating more coherent and informed interactions.\n",
      "\n",
      "Q: This sub-question focuses on understanding the specific mechanisms, techniques, or architectures LangChain employs to handle memory\n",
      "A: Based on the context provided, LangChain employs the following mechanisms to handle memory:\n",
      "\n",
      "1. **ConversationBufferMemory**: This module allows the LLM (Large Language Model) to maintain awareness of previous conversation turns.\n",
      "2. **ConversationSummaryMemory**: This module enables the LLM to summarize long interactions to fit within token limits.\n",
      "\n",
      "These memory modules are designed to help the LLM keep track of previous conversations and manage long interactions, ensuring that the model remains aware of the context and can respond accordingly. \n",
      "\n",
      "Additionally, LangChain's modular and composable workflows allow for the combination and reuse of components like retrievers, memories, agents, and chains, making it suitable for building scalable and maintainable LLM applications. \n",
      "\n",
      "This indicates that LangChain uses a combination of specific memory modules (ConversationBufferMemory and ConversationSummaryMemory) and a modular architecture to handle memory and context management.\n",
      "\n",
      "Q: **How does LangChain implement and use agents in its framework?**\n",
      "A: **LangChain implements and uses agents through a planner-executor model.** Here's a breakdown of how agents are used in the framework:\n",
      "\n",
      "* **Planning**: The agent plans out a sequence of tool invocations to achieve a specific goal. This planning process involves dynamic decision-making, branching logic, and context-aware memory use across steps.\n",
      "* **Execution**: The agent executes the planned sequence of tool invocations, using Large Language Models (LLMs) to reason about:\n",
      "\t+ Which tool to call\n",
      "\t+ What input to provide\n",
      "\t+ How to process the output\n",
      "* **Integration with tools**: LangChain agents can integrate with various tools, such as:\n",
      "\t+ Web search\n",
      "\t+ Calculators\n",
      "\t+ Code execution\n",
      "* **Multi-step task execution**: Agents can execute multi-step tasks, allowing them to perform complex tasks that require multiple actions.\n",
      "\n",
      "Overall, LangChain's agent framework enables the creation of sophisticated, autonomous agents that can reason, plan, and execute tasks to achieve specific goals.\n",
      "\n",
      "Q: This sub-question aims to clarify the role of agents within LangChain, including their implementation, functionalities, and interactions with other components like memory\n",
      "A: Based on the provided context, here's an answer to clarify the role of agents within LangChain:\n",
      "\n",
      "**Role of Agents in LangChain:**\n",
      "\n",
      "LangChain agents are components that utilize Large Language Models (LLMs) to reason about and execute tasks. They operate using a **planner-executor model**, which involves planning a sequence of tool invocations to achieve a specific goal. This planning process includes:\n",
      "\n",
      "1. **Dynamic decision-making**: Agents can make decisions based on the context and output of previous steps.\n",
      "2. **Branching logic**: Agents can adapt their plan based on the results of previous steps, allowing for flexible and dynamic execution.\n",
      "3. **Context-aware memory use**: Agents can utilize memory across multiple steps, enabling them to maintain context and make informed decisions.\n",
      "\n",
      "**Functionalities:**\n",
      "\n",
      "LangChain agents can:\n",
      "\n",
      "1. **Execute multi-step tasks**: Agents can perform complex tasks that involve multiple steps and interactions with various tools.\n",
      "2. **Integrate with tools**: Agents can interact with various tools, such as web search, calculators, and code execution, to achieve their goals.\n",
      "\n",
      "**Interactions with Other Components:**\n",
      "\n",
      "Agents interact with other components, including:\n",
      "\n",
      "1. **Memory**: Agents use context-aware memory to store and retrieve information across multiple steps, enabling them to maintain context and make informed decisions.\n",
      "2. **Tools**: Agents integrate with various tools to execute tasks and achieve their goals.\n",
      "\n",
      "Overall, LangChain agents play a crucial role in enabling the execution of complex tasks through their planner-executor model, dynamic decision-making, and interactions with other components like memory and tools.\n",
      "\n",
      "Q: **How do CrewAI's approaches to memory and agents differ from or compare to LangChain's?**\n",
      "A: Based on the provided context, here's a comparison of CrewAI's approaches to agents and how they differ from or compare to LangChain's:\n",
      "\n",
      "**Agents:**\n",
      "\n",
      "* **CrewAI:** Defines agents with a purpose, a goal, and a set of tools they can use. The framework ensures that each agent stays on task and contributes meaningfully to the overall crew objective, with a focus on role-based collaboration.\n",
      "* **LangChain:** While the context doesn't provide a detailed description of LangChain agents, it mentions that CrewAI is compatible with LangChain agents and tools. This implies that LangChain agents can be used within CrewAI's framework, but their specific characteristics or definitions are not provided.\n",
      "\n",
      "**Key differences:**\n",
      "\n",
      "* **Focus:** CrewAI focuses on role-based collaboration and task management, while LangChain seems to focus on retrieval and tool wrapping (as mentioned in the context).\n",
      "* **Agent management:** CrewAI ensures that each agent stays on task and contributes to the overall objective, whereas the context doesn't provide information on how LangChain manages its agents.\n",
      "\n",
      "**Comparison:**\n",
      "\n",
      "* **Hybrid systems:** Both CrewAI and LangChain can be used together to create hybrid systems, where LangChain handles retrieval and tool wrapping, and CrewAI manages role-based collaboration.\n",
      "\n",
      "Unfortunately, the context doesn't provide information on how CrewAI and LangChain approach memory. Therefore, a comparison of their memory approaches cannot be made based on the provided context.\n",
      "\n",
      "Q: Alternatively, this could be split into two separate questions for CrewAI's memory and agents, but combining them allows for a direct comparison:\n",
      "A: It seems like you forgot to include the actual question you'd like me to answer based on the provided context. Please go ahead and pose your question, and I'll do my best to assist you using the information about CrewAI agents and their functionalities.\n",
      "\n",
      "Q: How does CrewAI utilize memory in its operations?\n",
      "A: The provided context does not mention how CrewAI utilizes memory in its operations. The context only discusses the benefits and applications of CrewAI, a multi-agent orchestration framework, in complex tasks and multi-step workflows, but does not provide information about its memory utilization.\n",
      "\n",
      "Q: How does CrewAI implement and use agents in its framework?\n",
      "A: Based on the provided context, CrewAI implements and uses agents in its framework as follows:\n",
      "\n",
      "* Agents are defined with a specific **purpose**, **goal**, and a set of **tools** they can use.\n",
      "* Each agent has a defined **role**, such as researcher, planner, or executor.\n",
      "* Agents operate **semi-independently** within a collaborative context.\n",
      "* The framework ensures that each agent **stays on task** and contributes **meaningfully** to the overall **crew objective**.\n",
      "* Agents can form **structured workflows**, allowing them to work together in a coordinated and organized manner.\n",
      "\n",
      "Overall, CrewAI's framework enables agents to work together autonomously and collaboratively to achieve a common goal.\n",
      "\n",
      "Q: By addressing these sub-questions, one can gather detailed information on both LangChain and CrewAI's use of memory and agents, facilitating a comprehensive comparison\n",
      "A: It seems like you're setting up to ask a question related to the provided context about LangChain and CrewAI, but the actual question hasn't been posed yet. Based on the information given and the precursor to your question, I'll infer and answer:\n",
      "\n",
      "**Inferred Question:** How do LangChain and CrewAI utilize memory and agents, and what are their respective roles in hybrid systems?\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "1. **LangChain's Utilization of Memory and Agents:**\n",
      "   - LangChain agents operate using a planner-executor model. This model allows for planning a sequence of tool invocations to achieve a specific goal.\n",
      "   - LangChain agents can make dynamic decisions, implement branching logic, and use context-aware memory across different steps of their operation.\n",
      "   - In hybrid systems, LangChain's role is focused on handling retrieval and tool wrapping.\n",
      "\n",
      "2. **CrewAI's Utilization of Memory and Agents:**\n",
      "   - CrewAI is designed to manage role-based collaboration within systems.\n",
      "   - It is compatible with LangChain agents and tools, enabling the creation of hybrid systems.\n",
      "   - In these hybrid systems, CrewAI's primary role is to manage role-based collaboration, while LangChain focuses on retrieval and tool wrapping.\n",
      "\n",
      "3. **Comparison and Hybrid Systems:**\n",
      "   - **Memory Use:** The detailed use of memory, specifically how each platform stores, accesses, and utilizes memory for tasks like context-aware operations, isn't explicitly outlined in the provided context. However, LangChain agents are noted for their context-aware memory use across steps.\n",
      "   - **Agents and Roles:** LangChain agents seem to operate more autonomously with their planner-executor model, capable of dynamic decision-making. CrewAI, on the other hand, focuses on collaborative roles within a system, suggesting its strength lies in coordinating multiple agents or components to work together towards a goal.\n",
      "\n",
      "4. **Facilitating a Comprehensive Comparison:**\n",
      "   - To gather detailed information on both LangChain and CrewAI's use of memory and agents:\n",
      "     - Investigate LangChain's planner-executor model further and its implications on memory usage and task execution.\n",
      "     - Examine CrewAI's role-based collaboration capabilities and how it integrates with LangChain agents and tools.\n",
      "     - Analyze use cases or examples of hybrid systems leveraging both platforms to understand their practical applications and benefits.\n",
      "\n",
      "By delving into these areas, one can compile a comprehensive comparison that highlights the strengths, weaknesses, and optimal use cases for LangChain and CrewAI, especially concerning their use of memory and agents in various system architectures.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
