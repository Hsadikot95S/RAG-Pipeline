{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a129598",
      "metadata": {},
      "source": [
        "## Hybrid Retriever- Combining Dense And Sparse Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f518da96",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
        "\n",
        "# EnsembleRetriever not in langchain_community; implement weighted RRF locally\n",
        "class EnsembleRetriever(BaseRetriever):\n",
        "    \"\"\"Combine multiple retrievers with weighted Reciprocal Rank Fusion.\"\"\"\n",
        "    retrievers: list\n",
        "    weights: list\n",
        "    c: int = 60\n",
        "\n",
        "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None):\n",
        "        from langchain_core.documents import Document\n",
        "        doc_sets = [r.invoke(query) for r in self.retrievers]\n",
        "        fused = {}\n",
        "        for docs, w in zip(doc_sets, self.weights):\n",
        "            for rank, doc in enumerate(docs):\n",
        "                key = (doc.page_content, tuple(doc.metadata.items()) if doc.metadata else ())\n",
        "                fused[key] = fused.get(key, 0.0) + w / (self.c + rank + 1)\n",
        "        sorted_docs = sorted(fused.items(), key=lambda x: -x[1])\n",
        "        return [Document(page_content=c, metadata=dict(m)) for (c, m), _ in sorted_docs]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5c86da13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Sample documents\n",
        "docs = [\n",
        "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
        "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
        "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
        "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
        "    Document(page_content=\"Langchain has many types of retrievers.\")\n",
        "]\n",
        "\n",
        "# Step 2: Dense Retriever (FAISS + HuggingFace)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
        "dense_retriever = dense_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "76569a22",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Sparse Retriever(BM25)\n",
        "sparse_retriever=BM25Retriever.from_documents(docs)\n",
        "sparse_retriever.k=3 ##top- k documents to retriever\n",
        "\n",
        "## step 4 : Combine with Ensemble Retriever\n",
        "hybrid_retriever=EnsembleRetriever(\n",
        "    retrievers=[dense_retriever,sparse_retriever],\n",
        "    weights=[0.7,0.3]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "57d59933",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x129b7c550>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x129b7d900>, k=3)], weights=[0.7, 0.3])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hybrid_retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dec3b869",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”¹ Document 1:\n",
            "LangChain helps build LLM applications.\n",
            "\n",
            "ðŸ”¹ Document 2:\n",
            "Langchain can be used to develop agentic ai application.\n",
            "\n",
            "ðŸ”¹ Document 3:\n",
            "Langchain has many types of retrievers.\n",
            "\n",
            "ðŸ”¹ Document 4:\n",
            "Pinecone is a vector database for semantic search.\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Query and get results\n",
        "query = \"How can I build an application using LLMs?\"\n",
        "results = hybrid_retriever.invoke(query)\n",
        "\n",
        "# Step 6: Print results\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\nðŸ”¹ Document {i+1}:\\n{doc.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11c57cb",
      "metadata": {},
      "source": [
        "### RAG Pipeline with hybrid retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0bf22afb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models.base import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "99e17a99",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatOpenAI(profile={'max_input_tokens': 16385, 'max_output_tokens': 4096, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': False, 'structured_output': False, 'image_url_inputs': False, 'pdf_inputs': False, 'pdf_tool_message': False, 'image_tool_message': False, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x129ae52a0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x12b06de10>, root_client=<openai.OpenAI object at 0x12a8c77f0>, root_async_client=<openai.AsyncOpenAI object at 0x12b06dd50>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 5: Prompt Template (ChatPromptTemplate for chat models)\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the question based on the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {input}\n",
        "\"\"\")\n",
        "\n",
        "## step 6-llm\n",
        "llm = init_chat_model(\"openai:gpt-3.5-turbo\", temperature=0.2)\n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b9eb55e1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableAssign(mapper={\n",
              "  context: RunnableLambda(lambda x: hybrid_retriever.invoke(x['input']))\n",
              "})\n",
              "| RunnableAssign(mapper={\n",
              "    answer: RunnableLambda(lambda x: (prompt | llm | StrOutputParser()).invoke({'context': format_docs(x['context']), 'input': x['input']}))\n",
              "  })"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### RAG chain with LCEL (replaces create_stuff_documents_chain + create_retrieval_chain)\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain returns {\"context\": docs, \"answer\": ...} to match legacy create_retrieval_chain output\n",
        "rag_chain = (\n",
        "    RunnablePassthrough.assign(context=lambda x: hybrid_retriever.invoke(x[\"input\"]))\n",
        "    | RunnablePassthrough.assign(\n",
        "        answer=lambda x: (prompt | llm | StrOutputParser()).invoke(\n",
        "            {\"context\": format_docs(x[\"context\"]), \"input\": x[\"input\"]}\n",
        "        )\n",
        "    )\n",
        ")\n",
        "rag_chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6bb2441c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Answer:\n",
            " You can build an app using LLMs by utilizing LangChain, which helps in developing LLM applications. LangChain can be used to create agentic AI applications and has various types of retrievers to enhance the functionality of your app. Additionally, you can also consider using Pinecone, a vector database for semantic search, to further improve the performance of your LLM-based app.\n",
            "\n",
            "ðŸ“„ Source Documents:\n",
            "\n",
            "Doc 1: LangChain helps build LLM applications.\n",
            "\n",
            "Doc 2: Langchain can be used to develop agentic ai application.\n",
            "\n",
            "Doc 3: Langchain has many types of retrievers.\n",
            "\n",
            "Doc 4: Pinecone is a vector database for semantic search.\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Ask a question\n",
        "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
        "response = rag_chain.invoke(query)\n",
        "\n",
        "# Step 10: Output\n",
        "print(\"âœ… Answer:\\n\", response[\"answer\"])\n",
        "\n",
        "print(\"\\nðŸ“„ Source Documents:\")\n",
        "for i, doc in enumerate(response[\"context\"]):\n",
        "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c4468a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a58760",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb41501b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99604bae",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.10.19)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
